{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\DS-projects\\venv38-64\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import warnings\n",
    "from resources import *\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import metrics, losses, optimizers\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from keras import regularizers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\DS-projects\\venv38-64\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python resources.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load user items data\n",
    "# recdata = pd.read_csv('recdata.csv', index_col=0)\n",
    "# recdata = recdata.rename(columns={'variable': 'id', 'value': 'owned'})\n",
    "# recdata.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>70</th>\n",
       "      <th>80</th>\n",
       "      <th>130</th>\n",
       "      <th>220</th>\n",
       "      <th>...</th>\n",
       "      <th>526790</th>\n",
       "      <th>527340</th>\n",
       "      <th>527440</th>\n",
       "      <th>527510</th>\n",
       "      <th>527520</th>\n",
       "      <th>527810</th>\n",
       "      <th>527890</th>\n",
       "      <th>527900</th>\n",
       "      <th>528660</th>\n",
       "      <th>530720</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8657 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id   10      20      30      40      50      60      70      80      130     \\\n",
       "uid                                                                           \n",
       "0       1.0     1.0     1.0     1.0     1.0     1.0     1.0     0.0     1.0   \n",
       "1       1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "2       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3       1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "4       1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "\n",
       "id   220     ...  526790  527340  527440  527510  527520  527810  527890  \\\n",
       "uid          ...                                                           \n",
       "0       1.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1       1.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2       1.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3       1.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4       1.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "id   527900  528660  530720  \n",
       "uid                          \n",
       "0       0.0     0.0     0.0  \n",
       "1       0.0     0.0     0.0  \n",
       "2       0.0     0.0     0.0  \n",
       "3       0.0     0.0     0.0  \n",
       "4       0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 8657 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use create_interaction_matrix function\n",
    "data = create_interaction_matrix(\n",
    "    df=pd.read_csv('recdata.csv', index_col=0).rename(columns={'variable': 'id', 'value': 'owned'}),\n",
    "    user_col=\"uid\", item_col=\"id\", rating_col=\"owned\"\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user dictionary using helper function\n",
    "# user_dict = create_user_dict(interactions=interactions)\n",
    "\n",
    "# # Load games data\n",
    "# gamesdf = pd.read_csv('gamesdata.csv', index_col=0)\n",
    "\n",
    "# # Create game dictionary using helper function\n",
    "# games_dict = create_item_dict(df=gamesdf, id_col='id', name_col='title')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recs functions\n",
    "\n",
    "def get_game_title(game_id, game_data):\n",
    "    title = None\n",
    "    if game_id in game_data.index:\n",
    "        title = game_data.loc[game_id, 'title']\n",
    "    return title\n",
    "\n",
    "def get_recommendations(model, data, user_id, n, scaler, pca, item_titles):\n",
    "    # Get the index of the user in the data\n",
    "    user_index = data.index.get_loc(user_id)\n",
    "\n",
    "    # Get the user's interactions with the games\n",
    "    interactions = data.iloc[user_index].values\n",
    "\n",
    "    # Scale and reduce the interactions using the same scaler and PCA used for training\n",
    "    interactions_scaled = scaler.transform([interactions])\n",
    "    interactions_reduced = pca.transform(interactions_scaled)\n",
    "\n",
    "    # Generate the input features for the model\n",
    "    input_features = interactions_reduced.reshape(1, -1)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(input_features)[0]\n",
    "\n",
    "    # Get the indices of recommendations\n",
    "    top_indices = predictions.argsort()[::-1]\n",
    "\n",
    "\n",
    "    # Print the user's games and the top N recommendations\n",
    "    print(\"User's games:\")\n",
    "    user_games = data.iloc[user_index][data.iloc[user_index] != 0].index.tolist()\n",
    "\n",
    "    for game_id in user_games:\n",
    "        game_title = item_titles.get(game_id)\n",
    "        if game_title is not None:\n",
    "            print(\"- {}\".format(game_title))\n",
    "\n",
    "\n",
    "    # Get the titles of the top N recommendations\n",
    "    game_titles = []\n",
    "    for i in top_indices:\n",
    "        game_id = data.columns[i]\n",
    "        game_title = item_titles.get(game_id)\n",
    "        if game_title is not None and game_id not in user_games:\n",
    "            game_titles.append(game_title)    \n",
    "\n",
    "    print(\"Top recommendations:\")\n",
    "    for i, game_title in enumerate(game_titles):\n",
    "        print(\"{}. {}\".format(i + 1, game_title))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(y_true, y_pred, k=5):\n",
    "    # Sort the predicted scores in descending order and get the top k indices\n",
    "    _, indices = tf.math.top_k(y_pred, k=k)\n",
    "    \n",
    "    # Convert the indices to a boolean tensor indicating whether each game was in the top k\n",
    "    top_k = tf.reduce_any(tf.equal(indices, tf.expand_dims(y_true, axis=-1)), axis=-1)\n",
    "    \n",
    "    # Compute the recall by taking the mean of the boolean tensor\n",
    "    recall = tf.reduce_mean(tf.cast(top_k, dtype=tf.float32))\n",
    "    \n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    # Get top-k predicted items\n",
    "    y_pred_top_k = tf.math.top_k(y_pred, k=k).indices\n",
    "    # Convert predictions to binary 0/1 values\n",
    "    y_pred_binary = tf.cast(tf.greater(y_pred, 0.5), tf.int32)\n",
    "    # Calculate true positives by taking element-wise multiplication of y_true and y_pred_binary\n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.multiply(y_true, y_pred_binary), tf.float32), axis=1)\n",
    "    # Calculate precision at k as the ratio of true positives to k\n",
    "    precision_at_k = tf.reduce_mean(tf.divide(true_positives, k))\n",
    "    return precision_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22281, 8657)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data & create model\n",
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Create sparse matrices for evaluation\n",
    "train_sparse = sparse.csr_matrix(train_data.values)\n",
    "\n",
    "# Add X users to Test so that the number of rows in Train match Test\n",
    "N = train_data.shape[0]                      # Rows in Train set\n",
    "n, m = train_data.shape                       # Rows & columns in Test set\n",
    "z = np.zeros([(N - n), m])              # Create the necessary rows of zeros with m columns\n",
    "\n",
    "\n",
    "test = np.vstack((test_data, z))        # Vertically stack Test on top of the blank users\n",
    "test_sparse = sparse.csr_matrix(test)   # Convert back to sparse\n",
    "\n",
    "print(train_sparse.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(data.shape[1],)),\n",
    "    Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(data.shape[1], activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_data into input and target\n",
    "train_input = train_data.values\n",
    "train_target = train_sparse.todense()\n",
    "\n",
    "# Split test_data into input and target\n",
    "test_input = test_data.values\n",
    "test_target = test_sparse.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"e:\\DS-projects\\venv38-64\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\al\\AppData\\Local\\Temp\\ipykernel_16244\\3869483884.py\", line 20, in precision_at_k  *\n        true_positives = tf.reduce_sum(tf.cast(tf.multiply(y_true, y_pred_binary), tf.float32), axis=1)\n\n    TypeError: Input 'y' of 'Mul' Op has type int32 that does not match type float32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32me:\\DS-projects\\Steam-recs-experimental\\5-TF-Torch-Modeling.ipynb Cell 14\u001B[0m in \u001B[0;36m<cell line: 20>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001B[0m model\u001B[39m.\u001B[39mcompile(optimizer\u001B[39m=\u001B[39moptimizer, loss\u001B[39m=\u001B[39mloss, metrics\u001B[39m=\u001B[39mmetr)\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001B[0m \u001B[39m# Train the model on the training data\u001B[39;00m\n\u001B[1;32m---> <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001B[0m model\u001B[39m.\u001B[39;49mfit(\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001B[0m     train_input,\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001B[0m     train_target,\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001B[0m     epochs\u001B[39m=\u001B[39;49m\u001B[39m400\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001B[0m     batch_size\u001B[39m=\u001B[39;49m\u001B[39m16\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001B[0m     validation_data\u001B[39m=\u001B[39;49m(test_input, test_target),\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001B[0m     callbacks\u001B[39m=\u001B[39;49m[tensorboard_callback],\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/DS-projects/Steam-recs-experimental/5-TF-Torch-Modeling.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001B[0m )\n",
      "File \u001B[1;32me:\\DS-projects\\venv38-64\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[39m=\u001B[39m _process_traceback_frames(e\u001B[39m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[39m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[39m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[39mraise\u001B[39;00m e\u001B[39m.\u001B[39mwith_traceback(filtered_tb) \u001B[39mfrom\u001B[39;00m \u001B[39mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[39mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[39mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file62i1li40.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[39m=\u001B[39m ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(step_function), (ag__\u001B[39m.\u001B[39mld(\u001B[39mself\u001B[39m), ag__\u001B[39m.\u001B[39mld(iterator)), \u001B[39mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[39mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[39m=\u001B[39m \u001B[39mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqi7o211i.py:12\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__precision_at_k\u001B[1;34m(y_true, y_pred, k)\u001B[0m\n\u001B[0;32m     10\u001B[0m y_pred_top_k \u001B[39m=\u001B[39m ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mmath\u001B[39m.\u001B[39mtop_k, (ag__\u001B[39m.\u001B[39mld(y_pred),), \u001B[39mdict\u001B[39m(k\u001B[39m=\u001B[39mag__\u001B[39m.\u001B[39mld(k)), fscope)\u001B[39m.\u001B[39mindices\n\u001B[0;32m     11\u001B[0m y_pred_binary \u001B[39m=\u001B[39m ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mcast, (ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mgreater, (ag__\u001B[39m.\u001B[39mld(y_pred), \u001B[39m0.5\u001B[39m), \u001B[39mNone\u001B[39;00m, fscope), ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mint32), \u001B[39mNone\u001B[39;00m, fscope)\n\u001B[1;32m---> 12\u001B[0m true_positives \u001B[39m=\u001B[39m ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mreduce_sum, (ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mcast, (ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mmultiply, (ag__\u001B[39m.\u001B[39mld(y_true), ag__\u001B[39m.\u001B[39mld(y_pred_binary)), \u001B[39mNone\u001B[39;00m, fscope), ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mfloat32), \u001B[39mNone\u001B[39;00m, fscope),), \u001B[39mdict\u001B[39m(axis\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m), fscope)\n\u001B[0;32m     13\u001B[0m precision_at_k \u001B[39m=\u001B[39m ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mreduce_mean, (ag__\u001B[39m.\u001B[39mconverted_call(ag__\u001B[39m.\u001B[39mld(tf)\u001B[39m.\u001B[39mdivide, (ag__\u001B[39m.\u001B[39mld(true_positives), ag__\u001B[39m.\u001B[39mld(k)), \u001B[39mNone\u001B[39;00m, fscope),), \u001B[39mNone\u001B[39;00m, fscope)\n\u001B[0;32m     14\u001B[0m \u001B[39mtry\u001B[39;00m:\n",
      "\u001B[1;31mTypeError\u001B[0m: in user code:\n\n    File \"e:\\DS-projects\\venv38-64\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\al\\AppData\\Local\\Temp\\ipykernel_16244\\3869483884.py\", line 20, in precision_at_k  *\n        true_positives = tf.reduce_sum(tf.cast(tf.multiply(y_true, y_pred_binary), tf.float32), axis=1)\n\n    TypeError: Input 'y' of 'Mul' Op has type int32 that does not match type float32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "# Log tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "metr = (\n",
    "    precision_at_k,\n",
    "    recall_at_k\n",
    ")\n",
    "loss = (losses.BinaryCrossentropy())\n",
    "optimizer = optimizers.Adam(learning_rate=0.005)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metr)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(\n",
    "    train_input,\n",
    "    train_target,\n",
    "    epochs=400,\n",
    "    batch_size=16,\n",
    "    validation_data=(test_input, test_target),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "history = model.fit(train_input, train_target, epochs=10, batch_size=32, validation_data=(test_input, test_target))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_precision, test_recall = model.evaluate(test_input, test_target)\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test precision at k=10: {test_precision:.4f}\")\n",
    "print(f\"Test recall at k=10: {test_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......auc\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......precision\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......recall\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-03-14 16:44:10           64\n",
      "config.json                                    2023-03-14 16:44:10         2955\n",
      "variables.h5                                   2023-03-14 16:44:10       100312\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-03-14 16:44:10           64\n",
      "config.json                                    2023-03-14 16:44:10         2955\n",
      "variables.h5                                   2023-03-14 16:44:10       100312\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......auc\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......precision\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......recall\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "pickled_model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "User's games:\n",
      "- Counter-Strike\n",
      "- Team Fortress Classic\n",
      "- Half-Life: Source\n",
      "- Alien Swarm\n",
      "- Mare Nostrum\n",
      "- Iron Warriors: T - 72 Tank Command\n",
      "- Quake IV\n",
      "- QUAKE\n",
      "- QUAKE II\n",
      "- QUAKE II Mission Pack: The Reckoning\n",
      "- X: Tension\n",
      "- 688(I) Hunter/Killer\n",
      "- Xpand Rally\n",
      "- Bejeweled 2 Deluxe\n",
      "- Zuma Deluxe\n",
      "- Bejeweled Deluxe\n",
      "- Escape Rosecliff Island\n",
      "- Zuma's Revenge!\n",
      "- Garry's Mod\n",
      "- Silverfall\n",
      "- Company of Heroes - Legacy Edition\n",
      "- Indiana Jones® and the Fate of Atlantis™\n",
      "- Shank\n",
      "- Vegas: Make It Big™\n",
      "- Bloodline Champions\n",
      "- Commandos 2: Men of Courage\n",
      "- Deus Ex: Game of the Year Edition\n",
      "- Jade Empire™: Special Edition\n",
      "- Sid Meier's Railroads!\n",
      "- Railroad Tycoon II Platinum\n",
      "- Sam &amp; Max 106: Bright Side of the Moon\n",
      "- Sam &amp; Max 202: Moai Better Blues\n",
      "- Sam &amp; Max 204: Chariots of the Dogs\n",
      "- Telltale Texas Hold ‘Em\n",
      "- RACE 07: Andy Priaulx Crowne Plaza Raceway (Free DLC)\n",
      "- Civilization IV: Beyond the Sword\n",
      "- Freedom Force vs. the Third Reich\n",
      "- Champions Online\n",
      "- Haunted House™\n",
      "- Call of Duty: World at War\n",
      "- TimeShift™\n",
      "- A Stroke of Fate: Operation Valkyrie\n",
      "- Shadowgrounds Survivor\n",
      "- Gumboy Tournament\n",
      "- Grand Theft Auto\n",
      "- SlamIt Pinball Big Score\n",
      "- Ankh 3: Battle of the Gods\n",
      "- Mahjongg Investigations: Under Suspicion\n",
      "- Ride! Carnival Tycoon\n",
      "- Midnight Outlaw: 6 Hours to SunUp\n",
      "- Unreal Tournament 2004: Editor's Choice Edition\n",
      "- Prince of Persia: The Two Thrones™\n",
      "- Tom Clancy's Ghost Recon® Desert Siege™\n",
      "- Rayman Raving Rabbids™\n",
      "- Brothers in Arms: Road to Hill 30™\n",
      "- Silent Hunter®: Wolves of the Pacific\n",
      "- Heroes of Might &amp; Magic V: Hammers of Fate\n",
      "- The Wonderful End of the World\n",
      "- Oddworld: Abe's Exoddus®\n",
      "- Oddworld: Munch's Oddysee\n",
      "- Oddworld: Stranger's Wrath HD\n",
      "- Reaxxion\n",
      "- Little Farm\n",
      "- Discovery! A Seek and Find Adventure\n",
      "- Civilization V - Cradle of Civilization Map Pack: Americas\n",
      "- Civilization V - Cradle of Civilization Map Pack: Mesopotamia\n",
      "- Civilization V - Babylon (Nebuchadnezzar II)\n",
      "- Children of the Nile: Enhanced Edition\n",
      "- Crysis Warhead®\n",
      "- SPORE™ Creepy &amp; Cute Parts Pack\n",
      "- Pirates, Vikings, and Knights II\n",
      "- Nuclear Dawn\n",
      "- Crazy Machines 2: Liquid Force Add-on\n",
      "- Defense Grid: Containment DLC\n",
      "- Silent Hill Homecoming\n",
      "- Prince of Persia®\n",
      "- Warhammer® 40,000: Dawn of War® II Chaos Rising\n",
      "- The Witcher: Director's Cut Update\n",
      "- Pyroblazer®\n",
      "- Street Fighter® IV\n",
      "- World of Goo\n",
      "- Fallout 3: Game of the Year Edition\n",
      "- Fallout: New Vegas\n",
      "- Alien Breed: Impact\n",
      "- Gyromancer\n",
      "- Puzzle Quest: Galactrix\n",
      "- Train Simulator: Colton &amp; Northern Route Add-On\n",
      "- Medal of Honor: Airborne\n",
      "- Madballs Anarchy Unlock Pack\n",
      "- Hearts of Iron III: US Pack DLC\n",
      "- Hearts of Iron III: Japanese Infantry Pack DLC\n",
      "- Hearts of Iron III: DLC - German Sprite Pack\n",
      "- Guild Wars® Game of the Year Edition\n",
      "- Guild Wars Factions®\n",
      "- Guild Wars Trilogy\n",
      "Top recommendations:\n",
      "1. Half-Life: Opposing Force\n",
      "2. Deathmatch Classic\n",
      "3. Day of Defeat\n"
     ]
    }
   ],
   "source": [
    "user_id = 10\n",
    "num_recommendations = 5\n",
    "recommendations = get_recommendations(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    user_id=user_id,\n",
    "    n=num_recommendations,\n",
    "    scaler=scaler,\n",
    "    pca=pca,\n",
    "    item_titles=games_dict\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38-64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}